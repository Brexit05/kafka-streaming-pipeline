Crypto Streaming Pipeline

An end-to-end real-time data pipeline that ingests live cryptocurrency prices, streams them with Kafka, and stores processed data into a PostgreSQL database using Docker containers.

This project demonstrates how to build a scalable streaming data engineering pipeline, integrating producers, consumers, and persistent storage.

Architecture
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ CoinGecko  â”‚ ----> â”‚ Kafka Producer â”‚ ----> â”‚   Kafka     â”‚
         â”‚   API      â”‚       â”‚   (Python)    â”‚       â”‚   Broker    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚   Kafka Consumer   â”‚
                                            â”‚   (Python)         â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚   PostgreSQL DB    â”‚
                                            â”‚ (crypto_prices,    â”‚
                                            â”‚  crypto_alerts)    â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tech Stack

Python â†’ Kafka Producer & Consumer

Kafka â†’ Real-time event streaming

Zookeeper â†’ Kafka coordination

PostgreSQL â†’ Storage for processed data

Docker + Docker Compose â†’ Container orchestration

SQLAlchemy â†’ Database interaction

Workflow

Producer (producer.py)

Fetches live cryptocurrency prices from the CoinGecko API.

Publishes records to two Kafka topics:

crypto_prices â†’ regular price updates.

crypto_alerts â†’ alerts for coins with >5% volatility in 24h.

Kafka Broker

Handles event streaming between producer and consumer.

Consumer (consumer.py)

Subscribes to crypto_prices and crypto_alerts.

Inserts data into PostgreSQL tables:

crypto_prices (symbol, price, market cap, volume, change %, timestamp).

crypto_alerts (symbol, price, change %, alert type).

Database

PostgreSQL container persists the data.

Tables created via init.sql.

ðŸ“‚ Project Structure
kafka-streaming-project/
â”‚â”€â”€ docker-compose.yaml       # Service definitions (Kafka, Zookeeper, Postgres, Producer, Consumer)
â”‚â”€â”€ producer.py               # Kafka producer script
â”‚â”€â”€ consumer.py               # Kafka consumer script
â”‚â”€â”€ config.py                 # Config variables (URLs, DB, API params)
â”‚â”€â”€ init.sql                  # Initializes PostgreSQL schema
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ README.md                 # Project documentation
â””â”€â”€ .env                      # environment variables

How to Run
1. Clone the repository
git clone https://github.com/Brexit05/kafka-streaming-pipeline.git
cd kafka-streaming-project

2. Start Docker containers
docker-compose up -d

3. Check running services
docker ps

4. Follow producer logs
docker logs -f kafka-streaming-project-producer-1

5. Follow consumer logs
docker logs -f kafka-streaming-project-consumer-1

6. Access PostgreSQL
docker exec -it crypto_postgres psql -U crypto_user -d crypto_db

Example Query

(Get the 5 most recent crypto price updates)

SELECT symbol, price_usd, change_24h, timestamp
FROM crypto_prices
ORDER BY timestamp DESC
LIMIT 5;

Key Learnings

How to containerize a Kafka + Zookeeper + Postgres + Python workflow.

Setting up Kafka producers & consumers in Python.

Managing real-time + persistent storage in a structured database.

Handling schema consistency between stream and batch layers.


Next Steps
Add data visualization with Power BI or Grafana.

Extend to a batch + stream hybrid pipeline (Airflow + Kafka).


Deploy to a cloud environment (AWS/GCP/Azure).
